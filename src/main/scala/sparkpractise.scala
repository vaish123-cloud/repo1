import org.apache.commons.math3.geometry.spherical.oned.ArcsSet.Split
import org.apache.spark.SparkContext


//import org.apache.spark.SparkContext
//
object sparkpractise {
//  def main(args:Array[String]): Unit={
//
//  val sc = new SparkContext("local[*]","vaish1")
//  val rdd1 = sc.textFile("C:/Users/HP/Desktop/file1.txt")
////    rdd1.collect.foreach(println)
//  val rdd2=rdd1.flatMap(x=>x.split(" "))
//   // rdd2.collect.foreach(println)
//  val rdd3 = rdd2.map(x=>(x,1))
//    //rdd3.collect.foreach(println)
//
//    val rdd4 = rdd3.reduceByKey((x,y)=>x+y)
//   // rdd4.collect.foreach(println)
//val rdd5 = rdd4.sortBy(x=>x._2,false)
//    rdd5.collect.foreach(println)


//  }
//
//
  def main(arr:Array[String]): Unit = {
//    val sc = new SparkContext("local[*]","apptask1")
//    val rdd1 = sc.textFile("C:/Users/HP/Desktop/file1.txt")
//    //rdd1.collect.foreach(println)
//    val rdd2 = rdd1.flatMap(x=>x.split("\\s+"))
//   // rdd2.collect.foreach(println)
//    val rdd3 = rdd2.map(x=>(x,1))
//    //rdd3.collect.foreach(println)
//    val rdd4 = rdd3.reduceByKey((x,y)=>x+y)
//    //rdd4.collect.foreach(println)
//    val rdd5 = rdd4.sortBy(x=>x._2,false)
//    rdd5.collect.foreach(println)


    ///datastructure in rdd




  }
}


